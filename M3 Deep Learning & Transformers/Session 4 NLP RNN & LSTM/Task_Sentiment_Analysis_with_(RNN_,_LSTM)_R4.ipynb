{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis with RNN and LSTM on Yelp Polarity Dataset\n",
        "\n",
        "## Dataset\n",
        "[link](https://huggingface.co/datasets/fancyzhx/yelp_polarity\n",
        ")\n",
        "Dataset: **Yelp Polarity** (from HuggingFace)\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yelp_polarity\")\n",
        "```\n",
        "\n",
        "**Behind the scenes:**\n",
        "- HuggingFace automatically downloads and caches the dataset.\n",
        "- Ready-to-use `train` and `test` splits.\n",
        "\n",
        "---\n",
        "## Objective\n",
        "\n",
        "Build and compare **two sequential neural models** for sentiment classification:\n",
        "\n",
        "- **Simple RNN**\n",
        "- **LSTM**\n",
        "\n",
        "Task:\n",
        "- Predict sentiment polarity of reviews.\n",
        "- Compare performance and training behavior under identical preprocessing and training settings.\n",
        "\n",
        "---\n",
        "## Experiments Overview\n",
        "\n",
        "We conduct three experiments in this unified setup:\n",
        "\n",
        "1) **Baseline — TF-IDF + Logistic Regression**\n",
        "2) **Deep Learning — Simple RNN**\n",
        "3) **Deep Learning — LSTM**\n",
        "\n",
        "This ordering establishes a strong traditional baseline before moving to neural sequence models.\n",
        "\n",
        "---\n",
        "## Experiment 0 — Baseline: TF-IDF + Logistic Regression\n",
        "\n",
        "### Objective\n",
        "Establish a traditional machine learning baseline using sparse text features.\n",
        "\n",
        "### Method\n",
        "- Convert reviews to TF-IDF vectors.\n",
        "- Train a Logistic Regression classifier.\n",
        "- Evaluate on the same train/test split used later for RNN/LSTM.\n",
        "\n",
        "\n",
        "### Outputs\n",
        "- Accuracy and F1-score\n",
        "- Confusion Matrix\n",
        "- Training time\n",
        "\n",
        "---\n",
        "## Workflow\n",
        "\n",
        "### 1) Data Preprocessing (Shared for Both Models)\n",
        "\n",
        "Steps applied once and reused for both RNN and LSTM:\n",
        "\n",
        "- Load dataset (Text + Label)\n",
        "- Clean text (lowercase, remove punctuation)\n",
        "- Tokenize text\n",
        "- Pad sequences to a fixed length\n",
        "- Encode labels\n",
        "\n",
        "This ensures a **fair and controlled comparison**.\n",
        "\n",
        "---\n",
        "### 2) Model A — Simple RNN\n",
        "\n",
        "**Architecture:**\n",
        "- Embedding Layer\n",
        "- SimpleRNN Layer (32–64 units)\n",
        "- Dense Output Layer (sigmoid / softmax)\n",
        "\n",
        "**Compile Setup:**\n",
        "- Optimizer: Adam\n",
        "- Loss: Binary/Categorical Crossentropy\n",
        "- Metric: Accuracy\n",
        "\n",
        "**Training:**\n",
        "- 5–10 epochs\n",
        "- Validation split: 20%\n",
        "\n",
        "---\n",
        "### 3) Model B — LSTM\n",
        "\n",
        "**Architecture:**\n",
        "- Embedding Layer\n",
        "- LSTM Layer (64–128 units)\n",
        "- Dense Output Layer\n",
        "\n",
        "**Training setup identical to RNN** to maintain experimental consistency.\n",
        "\n",
        "---\n",
        "### 4) Evaluation\n",
        "\n",
        "For both models:\n",
        "\n",
        "- Test Accuracy\n",
        "- Confusion Matrix\n",
        "- Example Predictions\n",
        "\n",
        "**Comparison Table:**\n",
        "\n",
        "| Model | Accuracy | Training Time | Notes |\n",
        "|--------|-----------|---------------|-------|\n",
        "| Baseline(TF-IDF + LR) | XX% | ** | **** |\n",
        "| RNN | XX% | ** | **** |\n",
        "| LSTM | XX% | ** | **** |\n",
        "\n",
        "---\n",
        "### 5) User Input Prediction Demo\n",
        "\n",
        "After training both models:\n",
        "\n",
        "- User enters any review sentence.\n",
        "- Apply same tokenizer and padding.\n",
        "- Predict sentiment using both models.\n",
        "- Display results side-by-side.\n",
        "\n",
        "---\n",
        "## Key Learning Outcome\n",
        "\n",
        "This unified experiment demonstrates:\n",
        "\n",
        "- Why this baseline matters?\n",
        "- How RNN processes text sequences.\n",
        "- How LSTM improves long-term dependency learning.\n",
        "- Trade-off between **training speed vs accuracy**.\n",
        "\n",
        "---\n",
        "## Deliverables\n",
        "\n",
        "- Single notebook containing:\n",
        "  - Shared preprocessing\n",
        "  - RNN model\n",
        "  - LSTM model\n",
        "  - Unified evaluation\n",
        "  - Comparison table\n",
        "  - Interactive prediction demo\n",
        "\n"
      ],
      "metadata": {
        "id": "RmhwpUf2pxga"
      },
      "id": "RmhwpUf2pxga"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHwLNn2Apw05"
      },
      "id": "GHwLNn2Apw05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNxOu-rOpCh4"
      },
      "id": "kNxOu-rOpCh4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}