{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7549,
     "status": "ok",
     "timestamp": 1758114708417,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "C9VtiFtIzUk-",
    "outputId": "495f14de-8db2-4a19-dc4b-73c9bdb8f679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1758126390699,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "VuyQNtxgzQ5U",
    "outputId": "33737cba-477c-488e-c7fa-2783b547dbc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method download in module nltk.downloader:\n",
      "\n",
      "download(info_or_id=None, download_dir=None, quiet=False, force=False, prefix='[nltk_data] ', halt_on_error=True, raise_on_error=False, print_error_to=<ipykernel.iostream.OutStream object at 0x7ffa01b53b50>) method of nltk.downloader.Downloader instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "help(nltk.download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1758126392499,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "hQ9viRtWzmwl",
    "outputId": "37d39a15-a804-482e-e64e-8559140ba015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Package abc>\n",
      "<Package alpino>\n",
      "<Package averaged_perceptron_tagger>\n",
      "<Package averaged_perceptron_tagger_eng>\n",
      "<Package averaged_perceptron_tagger_ru>\n",
      "<Package averaged_perceptron_tagger_rus>\n",
      "<Package basque_grammars>\n",
      "<Package bcp47>\n",
      "<Package biocreative_ppi>\n",
      "<Package bllip_wsj_no_aux>\n",
      "<Package book_grammars>\n",
      "<Package brown>\n",
      "<Package brown_tei>\n",
      "<Package cess_cat>\n",
      "<Package cess_esp>\n",
      "<Package chat80>\n",
      "<Package city_database>\n",
      "<Package cmudict>\n",
      "<Package comparative_sentences>\n",
      "<Package comtrans>\n",
      "<Package conll2000>\n",
      "<Package conll2002>\n",
      "<Package conll2007>\n",
      "<Package crubadan>\n",
      "<Package dependency_treebank>\n",
      "<Package dolch>\n",
      "<Package english_wordnet>\n",
      "<Package europarl_raw>\n",
      "<Package extended_omw>\n",
      "<Package floresta>\n",
      "<Package framenet_v15>\n",
      "<Package framenet_v17>\n",
      "<Package gazetteers>\n",
      "<Package genesis>\n",
      "<Package gutenberg>\n",
      "<Package ieer>\n",
      "<Package inaugural>\n",
      "<Package indian>\n",
      "<Package jeita>\n",
      "<Package kimmo>\n",
      "<Package knbc>\n",
      "<Package large_grammars>\n",
      "<Package lin_thesaurus>\n",
      "<Package mac_morpho>\n",
      "<Package machado>\n",
      "<Package masc_tagged>\n",
      "<Package maxent_ne_chunker>\n",
      "<Package maxent_ne_chunker_tab>\n",
      "<Package maxent_treebank_pos_tagger>\n",
      "<Package maxent_treebank_pos_tagger_tab>\n",
      "<Package mock_corpus>\n",
      "<Package moses_sample>\n",
      "<Package movie_reviews>\n",
      "<Package mte_teip5>\n",
      "<Package mwa_ppdb>\n",
      "<Package names>\n",
      "<Package nombank.1.0>\n",
      "<Package nonbreaking_prefixes>\n",
      "<Package nps_chat>\n",
      "<Package omw>\n",
      "<Package omw-1.4>\n",
      "<Package opinion_lexicon>\n",
      "<Package panlex_swadesh>\n",
      "<Package paradigms>\n",
      "<Package pe08>\n",
      "<Package perluniprops>\n",
      "<Package pil>\n",
      "<Package pl196x>\n",
      "<Package porter_test>\n",
      "<Package ppattach>\n",
      "<Package problem_reports>\n",
      "<Package product_reviews_1>\n",
      "<Package product_reviews_2>\n",
      "<Package propbank>\n",
      "<Package pros_cons>\n",
      "<Package ptb>\n",
      "<Package punkt>\n",
      "<Package punkt_tab>\n",
      "<Package qc>\n",
      "<Package reuters>\n",
      "<Package rslp>\n",
      "<Package rte>\n",
      "<Package sample_grammars>\n",
      "<Package semcor>\n",
      "<Package senseval>\n",
      "<Package sentence_polarity>\n",
      "<Package sentiwordnet>\n",
      "<Package shakespeare>\n",
      "<Package sinica_treebank>\n",
      "<Package smultron>\n",
      "<Package snowball_data>\n",
      "<Package spanish_grammars>\n",
      "<Package state_union>\n",
      "<Package stopwords>\n",
      "<Package subjectivity>\n",
      "<Package swadesh>\n",
      "<Package switchboard>\n",
      "<Package tagsets>\n",
      "<Package tagsets_json>\n",
      "<Package timit>\n",
      "<Package toolbox>\n",
      "<Package treebank>\n",
      "<Package twitter_samples>\n",
      "<Package udhr>\n",
      "<Package udhr2>\n",
      "<Package unicode_samples>\n",
      "<Package universal_tagset>\n",
      "<Package universal_treebanks_v20>\n",
      "<Package vader_lexicon>\n",
      "<Package verbnet>\n",
      "<Package verbnet3>\n",
      "<Package webtext>\n",
      "<Package wmt15_eval>\n",
      "<Package word2vec_sample>\n",
      "<Package wordnet>\n",
      "<Package wordnet2021>\n",
      "<Package wordnet2022>\n",
      "<Package wordnet31>\n",
      "<Package wordnet_ic>\n",
      "<Package words>\n",
      "<Package ycoe>\n"
     ]
    }
   ],
   "source": [
    "from nltk import downloader\n",
    "d = downloader.Downloader()\n",
    "for pkg in d.packages():\n",
    "    print(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1758126382811,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "NTNs3AnW4xcS",
    "outputId": "eaf77527-dccf-4095-f6ed-886459229e71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Download the 'punkt' resource (tokenizer)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFciy_ce4xcW"
   },
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
    "Please do watch the entire course. to become expert in NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNy96rdJ4xcX",
    "outputId": "44f3909e-5013-4c13-d97b-fd33688ca801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
      "Please do watch the entire course. to become expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdafQpMt4xcX"
   },
   "outputs": [],
   "source": [
    "##  Tokenization\n",
    "## Sentence-->paragraphs\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv-Lja1C4xcZ"
   },
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1758126434529,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "xZ6mVrr04xcZ",
    "outputId": "35f8bff0-9775-4c36-e8d2-562e3740ac69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Welcome,to Mohamed Atef NLP Tutorials.',\n",
       " 'Please do watch the entire course.',\n",
       " 'to become expert in NLP.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1758126463100,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "sITxKp5-4xcZ",
    "outputId": "c00ce2a0-98a0-469f-b07a-8f553baf12af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
      "1 Please do watch the entire course.\n",
      "2 to become expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "for sentence in documents:\n",
    "    print(num,sentence)\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCaHjO2g4xca"
   },
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Paragraph-->words\n",
    "## sentence--->words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1758126507500,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "drltLVzk4xca",
    "outputId": "b218b25a-e348-49ee-9137-69780ebabf72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Mohamed',\n",
       " 'Atef',\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '.',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1758126513523,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "maAQXDOc4xcb",
    "outputId": "87050bd7-8054-49a7-a10c-334c7544fa6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Mohamed', 'Atef', 'NLP', 'Tutorials', '.']\n",
      "['Please', 'do', 'watch', 'the', 'entire', 'course', '.']\n",
      "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMRpIcOyG-cX"
   },
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1125,
     "status": "ok",
     "timestamp": 1758126833760,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "5Gdg0WuaGt60",
    "outputId": "034bb5d9-d825-413c-b8e9-ada2e8bd1cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: [Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
      ", Please do watch the entire course., to become expert in NLP.\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define your text\n",
    "text = \"\"\"Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
    "Please do watch the entire course. to become expert in NLP.\n",
    "\"\"\"\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "# Tokenize into sentences\n",
    "sentences = [sent for sent in doc.sents]\n",
    "print(\"Sentence Tokens:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLmT0X3FH77I"
   },
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1758126837054,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "JMKDVVtKHB4S",
    "outputId": "7757eadf-83bd-4ad4-d87b-5f2d7d12b081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: [Hello, Welcome, ,, to, Mohamed, Atef, NLP, Tutorials, ., \n",
      ", Please, do, watch, the, entire, course, ., to, become, expert, in, NLP, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define your text\n",
    "text =\"\"\"Hello Welcome,to Mohamed Atef NLP Tutorials.\n",
    "Please do watch the entire course. to become expert in NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize into words\n",
    "words = [token for token in doc]\n",
    "print(\"Word Tokens:\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WgNlh1d4mpZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqklW9FHSFdK"
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1758127557367,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "FoexrY39SE2a",
    "outputId": "1944fcfb-e2d2-4cf9-985e-630e380b040e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['cared', 'university', 'fairly', 'easily', 'singing', 'sings', 'sung', 'singer', 'sportingly', 'congratulations']\n",
      "Porter Stemmer: ['care', 'univers', 'fairli', 'easili', 'sing', 'sing', 'sung', 'singer', 'sportingli', 'congratul']\n",
      "Snowball Stemmer: ['care', 'univers', 'fair', 'easili', 'sing', 'sing', 'sung', 'singer', 'sport', 'congratul']\n",
      "Lancaster Stemmer: ['car', 'univers', 'fair', 'easy', 'sing', 'sing', 'sung', 'sing', 'sport', 'congrat']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "### Snowball Stemmer\n",
    "# It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['cared','university','fairly','easily','singing',\n",
    "       'sings','sung','singer','sportingly','congratulations']\n",
    "\n",
    "porter_stems = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "snowball_stems = [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "lancaster_stems = [lancaster_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Porter Stemmer:\", porter_stems)\n",
    "print(\"Snowball Stemmer:\", snowball_stems)\n",
    "print(\"Lancaster Stemmer:\", lancaster_stems)\n",
    "# congratulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758116376107,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "eB6DaF2m5rAW",
    "outputId": "f96d4cfc-006d-4768-f8a6-d8289b035a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "wa\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"better\"))\n",
    "print(stemmer.stem(\"was\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo_eaxxNa_CA"
   },
   "source": [
    "## Wordnet Lemmatizer\n",
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
    "\n",
    "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWnRBd2OVJZY",
    "outputId": "61ee8df9-8f6d-46b3-c8b6-9ee3bbe05983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats: cat\n",
      "running: running\n",
      "better: better\n",
      "flies: fly\n",
      "congratulations: congratulation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Q&A,chatbots,text summarization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['cats', 'running', 'better', 'flies','congratulations']\n",
    "for word in words:\n",
    "    print(f\"{word}: {lemmatizer.lemmatize(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758116138131,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "4UDVulg-4ysw",
    "outputId": "506a684c-fa4f-41f9-fda8-0b76776d10f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet');\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize(\"better\", pos=\"a\"))   # → good\n",
    "print(lemma.lemmatize(\"was\", pos=\"v\"))      # → be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1758127886996,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "Tr78F3ip4-7n",
    "outputId": "2f95ba2d-914a-4549-c60d-04e4b924c34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better as Nono:  better\n",
      "better as adjective:  good\n",
      "was as Nono:  wa\n",
      "was as verb:  be\n"
     ]
    }
   ],
   "source": [
    "#\"n\" → noun\n",
    "# \"v\" → verb\n",
    "# \"a\" → adjective\n",
    "# \"r\" → adverb\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(\"better as Nono: \",lemma.lemmatize(\"better\"))             # بدون pos → better (اعتبرها noun)\n",
    "print(\"better as adjective: \",lemma.lemmatize(\"better\", pos=\"a\"))    # كصفة adjective → good\n",
    "\n",
    "print(\"was as Nono: \",lemma.lemmatize(\"was\"))                # بدون pos → was (ما قدر يرجع للجذر)\n",
    "print(\"was as verb: \",lemma.lemmatize(\"was\", pos=\"v\"))       # كفعل verb → be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEkmlDVxeAXB"
   },
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJDkB7cK2y8K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1768045493583,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -120
    },
    "id": "D2dD1khZSKG8",
    "outputId": "5dc219dc-9435-4796-fe0f-035d64cbdaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words len :  399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'have',\n",
       " 'three',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'india',\n",
       " '.',\n",
       " 'in',\n",
       " '3000',\n",
       " 'years',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history',\n",
       " ',',\n",
       " 'people',\n",
       " 'from',\n",
       " 'all',\n",
       " 'over',\n",
       " 'the',\n",
       " 'world',\n",
       " 'have',\n",
       " 'come',\n",
       " 'and',\n",
       " 'invaded',\n",
       " 'us',\n",
       " ',',\n",
       " 'captured',\n",
       " 'our',\n",
       " 'lands',\n",
       " ',',\n",
       " 'conquered',\n",
       " 'our',\n",
       " 'minds',\n",
       " '.',\n",
       " 'from',\n",
       " 'alexander',\n",
       " 'onwards',\n",
       " ',',\n",
       " 'the',\n",
       " 'greeks',\n",
       " ',',\n",
       " 'the',\n",
       " 'turks',\n",
       " ',',\n",
       " 'the',\n",
       " 'moguls',\n",
       " ',',\n",
       " 'the',\n",
       " 'portuguese',\n",
       " ',',\n",
       " 'the',\n",
       " 'british',\n",
       " ',',\n",
       " 'the',\n",
       " 'french',\n",
       " ',',\n",
       " 'the',\n",
       " 'dutch',\n",
       " ',',\n",
       " 'all',\n",
       " 'of',\n",
       " 'them',\n",
       " 'came',\n",
       " 'and',\n",
       " 'looted',\n",
       " 'us',\n",
       " ',',\n",
       " 'took',\n",
       " 'over',\n",
       " 'what',\n",
       " 'was',\n",
       " 'ours',\n",
       " '.',\n",
       " 'yet',\n",
       " 'we',\n",
       " 'have',\n",
       " 'not',\n",
       " 'done',\n",
       " 'this',\n",
       " 'to',\n",
       " 'any',\n",
       " 'other',\n",
       " 'nation',\n",
       " '.',\n",
       " 'we',\n",
       " 'have',\n",
       " 'not',\n",
       " 'conquered',\n",
       " 'anyone',\n",
       " '.',\n",
       " 'we',\n",
       " 'have',\n",
       " 'not',\n",
       " 'grabbed',\n",
       " 'their',\n",
       " 'land',\n",
       " ',',\n",
       " 'their',\n",
       " 'culture',\n",
       " ',',\n",
       " 'their',\n",
       " 'history',\n",
       " 'and',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'enforce',\n",
       " 'our',\n",
       " 'way',\n",
       " 'of',\n",
       " 'life',\n",
       " 'on',\n",
       " 'them',\n",
       " '.',\n",
       " 'why',\n",
       " '?',\n",
       " 'because',\n",
       " 'we',\n",
       " 'respect',\n",
       " 'the',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'others.that',\n",
       " 'is',\n",
       " 'why',\n",
       " 'my',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'is',\n",
       " 'that',\n",
       " 'of',\n",
       " 'freedom',\n",
       " '.',\n",
       " 'i',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'india',\n",
       " 'got',\n",
       " 'its',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'of',\n",
       " 'this',\n",
       " 'in',\n",
       " '1857',\n",
       " ',',\n",
       " 'when',\n",
       " 'we',\n",
       " 'started',\n",
       " 'the',\n",
       " 'war',\n",
       " 'of',\n",
       " 'independence',\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'this',\n",
       " 'freedom',\n",
       " 'that',\n",
       " 'we',\n",
       " 'must',\n",
       " 'protect',\n",
       " 'and',\n",
       " 'nurture',\n",
       " 'and',\n",
       " 'build',\n",
       " 'on',\n",
       " '.',\n",
       " 'if',\n",
       " 'we',\n",
       " 'are',\n",
       " 'not',\n",
       " 'free',\n",
       " ',',\n",
       " 'no',\n",
       " 'one',\n",
       " 'will',\n",
       " 'respect',\n",
       " 'us',\n",
       " '.',\n",
       " 'my',\n",
       " 'second',\n",
       " 'vision',\n",
       " 'for',\n",
       " 'india',\n",
       " '’',\n",
       " 's',\n",
       " 'development',\n",
       " '.',\n",
       " 'for',\n",
       " 'fifty',\n",
       " 'years',\n",
       " 'we',\n",
       " 'have',\n",
       " 'been',\n",
       " 'a',\n",
       " 'developing',\n",
       " 'nation',\n",
       " '.',\n",
       " 'it',\n",
       " 'is',\n",
       " 'time',\n",
       " 'we',\n",
       " 'see',\n",
       " 'ourselves',\n",
       " 'as',\n",
       " 'a',\n",
       " 'developed',\n",
       " 'nation',\n",
       " '.',\n",
       " 'we',\n",
       " 'are',\n",
       " 'among',\n",
       " 'the',\n",
       " 'top',\n",
       " '5',\n",
       " 'nations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'gdp',\n",
       " '.',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " '10',\n",
       " 'percent',\n",
       " 'growth',\n",
       " 'rate',\n",
       " 'in',\n",
       " 'most',\n",
       " 'areas',\n",
       " '.',\n",
       " 'our',\n",
       " 'poverty',\n",
       " 'levels',\n",
       " 'are',\n",
       " 'falling',\n",
       " '.',\n",
       " 'our',\n",
       " 'achievements',\n",
       " 'are',\n",
       " 'being',\n",
       " 'globally',\n",
       " 'recognised',\n",
       " 'today',\n",
       " '.',\n",
       " 'yet',\n",
       " 'we',\n",
       " 'lack',\n",
       " 'the',\n",
       " 'self-confidence',\n",
       " 'to',\n",
       " 'see',\n",
       " 'ourselves',\n",
       " 'as',\n",
       " 'a',\n",
       " 'developed',\n",
       " 'nation',\n",
       " ',',\n",
       " 'self-reliant',\n",
       " 'and',\n",
       " 'self-assured',\n",
       " '.',\n",
       " 'isn',\n",
       " '’',\n",
       " 't',\n",
       " 'this',\n",
       " 'incorrect',\n",
       " '?',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'third',\n",
       " 'vision',\n",
       " '.',\n",
       " 'india',\n",
       " 'must',\n",
       " 'stand',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'because',\n",
       " 'i',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'unless',\n",
       " 'india',\n",
       " 'stands',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'no',\n",
       " 'one',\n",
       " 'will',\n",
       " 'respect',\n",
       " 'us',\n",
       " '.',\n",
       " 'only',\n",
       " 'strength',\n",
       " 'respects',\n",
       " 'strength',\n",
       " '.',\n",
       " 'we',\n",
       " 'must',\n",
       " 'be',\n",
       " 'strong',\n",
       " 'not',\n",
       " 'only',\n",
       " 'as',\n",
       " 'a',\n",
       " 'military',\n",
       " 'power',\n",
       " 'but',\n",
       " 'also',\n",
       " 'as',\n",
       " 'an',\n",
       " 'economic',\n",
       " 'power',\n",
       " '.',\n",
       " 'both',\n",
       " 'must',\n",
       " 'go',\n",
       " 'hand-in-hand',\n",
       " '.',\n",
       " 'my',\n",
       " 'good',\n",
       " 'fortune',\n",
       " 'was',\n",
       " 'to',\n",
       " 'have',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'three',\n",
       " 'great',\n",
       " 'minds',\n",
       " '.',\n",
       " 'dr.',\n",
       " 'vikram',\n",
       " 'sarabhai',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dept',\n",
       " '.',\n",
       " 'of',\n",
       " 'space',\n",
       " ',',\n",
       " 'professor',\n",
       " 'satish',\n",
       " 'dhawan',\n",
       " ',',\n",
       " 'who',\n",
       " 'succeeded',\n",
       " 'him',\n",
       " 'and',\n",
       " 'dr.',\n",
       " 'brahm',\n",
       " 'prakash',\n",
       " ',',\n",
       " 'father',\n",
       " 'of',\n",
       " 'nuclear',\n",
       " 'material',\n",
       " '.',\n",
       " 'i',\n",
       " 'was',\n",
       " 'lucky',\n",
       " 'to',\n",
       " 'have',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'all',\n",
       " 'three',\n",
       " 'of',\n",
       " 'them',\n",
       " 'closely',\n",
       " 'and',\n",
       " 'consider',\n",
       " 'this',\n",
       " 'the',\n",
       " 'great',\n",
       " 'opportunity',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life',\n",
       " '.',\n",
       " 'i',\n",
       " 'see',\n",
       " 'four',\n",
       " 'milestones',\n",
       " 'in',\n",
       " 'my',\n",
       " 'career']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
    "               the world have come and invaded us, captured our lands, conquered our minds.\n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
    "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
    "               We have not grabbed their land, their culture,\n",
    "               their history and tried to enforce our way of life on them.\n",
    "               Why? Because we respect the freedom of others.That is why my\n",
    "               first vision is that of freedom. I believe that India got its first vision of\n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "               I see four milestones in my career\"\"\"\n",
    "\n",
    "paragraph = paragraph.lower()\n",
    "\n",
    "words = word_tokenize(paragraph)\n",
    "print(\"words len : \",len(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1768045556354,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -120
    },
    "id": "wJLTYBg5uW4_",
    "outputId": "509043d6-e363-4ef6-e096-ad5ba85b9d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you'll\", 'because', 'any', 'shan', 'too', \"haven't\", \"doesn't\", 'for', 'they', 'more', 'at', \"i'm\", 'your', 'isn', \"it'll\", \"you've\", 'nor', 'been', 'can', 'during', 'yourself', 'theirs', 've', \"wouldn't\", 'between', \"we're\", 'will', 'not', 'most', 'him', \"she'll\", 'their', 'wouldn', 'further', 'o', 'into', 'ourselves', 'had', 'over', 'themselves', 'myself', 'hadn', \"it'd\", 'the', 'yours', 'as', 'those', 'having', 'all', \"you're\", \"it's\", 'it', 'herself', 'about', \"i've\", 'be', \"didn't\", 'so', 'under', \"that'll\", 'once', 'with', 'above', 'my', 'he', 'here', 'am', 'does', \"should've\", 'through', 'was', \"we'll\", 'who', 'an', 'down', \"they'd\", \"won't\", 'aren', 'if', 'mustn', 'me', 'y', 'won', 'do', \"he'd\", 'himself', \"she'd\", \"you'd\", 'hasn', 'after', \"mightn't\", 'being', 'did', 'll', 'by', 'no', 'hers', 'on', 'from', 'now', 'm', 'then', 'other', \"i'd\", 'doesn', 'or', \"shan't\", 'than', 'below', 'has', 'wasn', \"wasn't\", 'where', \"isn't\", 'she', \"needn't\", 'are', 'ours', 'only', 't', 'which', 'how', \"hadn't\", 'ain', 'a', 'mightn', 'have', 'shouldn', 'were', 'while', 'this', 'why', 'haven', 'ma', 'few', 'before', \"i'll\", \"weren't\", 'of', \"hasn't\", 'same', 'doing', 'some', 'i', 'very', 'and', \"he'll\", 'both', 'these', 'just', 'what', 'up', 'to', 'don', 'them', 'until', \"he's\", 's', 'is', \"don't\", 'its', 'her', 'weren', 'whom', \"shouldn't\", 'but', \"mustn't\", \"aren't\", 'his', \"they're\", 'our', 'couldn', \"she's\", 'you', 'in', 'there', 'didn', 'against', 'again', 'that', \"we've\", \"they've\", 'when', 'off', 'needn', 'own', 'yourselves', \"couldn't\", 'we', 'd', 'each', 'itself', 'out', \"they'll\", 're', \"we'd\", 'such', 'should'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = {\"the\",'and',\"or\"}\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIG4Mg8JuYLX"
   },
   "outputs": [],
   "source": [
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "for word in words:\n",
    "    if word.isalnum() and word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P15IdQZouQ2H"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"lemmatized_words len : \",len(lemmatized_words))\n",
    "\n",
    "processed_paragraph = ' '.join(lemmatized_words)\n",
    "\n",
    "print(processed_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR3WavgFuMDQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1758128609499,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "aN1rFwkxoZD-",
    "outputId": "671dd925-e581-4ce1-fd12-38d6063624bb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'three vision india 3000 year history people world come invaded u captured land conquered mind alexander onwards greek turk mogul portuguese british french dutch came looted u took yet done nation conquered anyone grabbed land culture history tried enforce way life respect freedom first vision freedom believe india got first vision 1857 started war independence freedom must protect nurture build free one respect u second vision india development fifty year developing nation time see developed nation among top 5 nation world term gdp 10 percent growth rate area poverty level falling achievement globally recognised today yet lack see developed nation incorrect third vision india must stand world believe unless india stand world one respect u strength respect strength must strong military power also economic power must go good fortune worked three great mind vikram sarabhai dept space professor satish dhawan succeeded brahm prakash father nuclear material lucky worked three closely consider great opportunity life see four milestone career'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1758128477637,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "sN4nNIWHnbf2",
    "outputId": "1a659691-833f-470e-ddec-cac9912998eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1758128517608,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "fqdyrqOBnyO5",
    "outputId": "a1c81ee4-470b-432f-cb9d-4c5ad643564e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1758116737313,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "cVsklYBQ6v-l",
    "outputId": "0de25224-22a3-47aa-dc03-357490aa6548"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1758116751913,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "HZ7nAv777IpF",
    "outputId": "6d79fe84-4ffa-4a9d-cc93-f362e36fe02c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me3ArlcW8mE7"
   },
   "source": [
    "# POS(Part of speech)\n",
    "\n",
    "1- token.pos_ >>>>>>>represent pos(verp, noun,....)\n",
    "\n",
    "2- spacy.explain(token.pos_)   >>>>>akes the part-of-speech tag as an argument and returns a brief explanation of what the pos represents.\n",
    "\n",
    "3- token.tag_  >>>> tell me the tense(الزمن  بتاعى )\n",
    "\n",
    "4- spacy.explain(token.tag_ ) >>>> returns a brief explanation of what the tag represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 504,
     "status": "ok",
     "timestamp": 1758128765602,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "2QHDRP5hjpJT",
    "outputId": "d920a6af-628b-41b7-ecc3-2e0ad29a02b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon || PROPN || proper noun\n",
      "flew || VERB || verb\n",
      "to || ADP || adposition\n",
      "mars || NOUN || noun\n",
      "yesterday || NOUN || noun\n",
      ". || PUNCT || punctuation\n",
      "He || PRON || pronoun\n",
      "carried || VERB || verb\n",
      "biryani || ADJ || adjective\n",
      "masala || NOUN || noun\n",
      "with || ADP || adposition\n",
      "him || PRON || pronoun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\"||\", token.pos_,\"||\", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1758128782946,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "n450mhf08pjn",
    "outputId": "68bcca27-6478-4086-909a-23693e921dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow  |  INTJ  |  interjection  |  UH  |  interjection\n",
      "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
      "Dr.  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "Strange  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
      "made  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
      "265  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "million  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "$  |  NUM  |  numeral  |  CD  |  cardinal number\n",
      "on  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
      "the  |  DET  |  determiner  |  DT  |  determiner\n",
      "very  |  ADV  |  adverb  |  RB  |  adverb\n",
      "first  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "day  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1758128824780,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "ApySsqsApAGR",
    "outputId": "df2b1083-0ba9-4c90-b660-3b40f6d6eaeb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1fc78e965b2e4087bc2c5c55acecffa5-0\" class=\"displacy\" width=\"2150\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Wow!</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Dr.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Strange</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">made</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">265</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">million</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">very</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">first</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">day</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-0\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-2\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-3\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-4\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-5\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-6\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1970.0,89.5 1970.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-7\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-8\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,177.0 1965.0,177.0 1965.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,266.5 L1812,254.5 1828,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1975.0,2.0 1975.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1fc78e965b2e4087bc2c5c55acecffa5-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1975.0,266.5 L1983.0,254.5 1967.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"dep\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeEzlVzi820l"
   },
   "source": [
    "## using pos as a filter for Removing all SPACE, PUNCT and X token from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK0bSY2T8xr-"
   },
   "outputs": [],
   "source": [
    "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.7 billion and increased 20%\n",
    "·         Operating income was $22.2 billion and increased 24%\n",
    "·         Net income was $18.8 billion and increased 21%\n",
    "·         Diluted earnings per share was $2.48 and increased 22%\n",
    "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
    "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
    "\n",
    "doc = nlp(earnings_text)\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"PUNCT\",\"NOUN\",\"VERB\"]:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758129188483,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "DzSFDnFjqJ1q",
    "outputId": "538b689f-45d5-45eb-f7ea-afbed6561fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "2021\n",
      "51.7\n",
      "billion\n",
      "20\n",
      "22.2\n",
      "billion\n",
      "24\n",
      "18.8\n",
      "billion\n",
      "21\n",
      "2.48\n",
      "22\n",
      "22.1\n",
      "billion\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_  in [\"NUM\"]:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1758129008706,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "NUHWpAhL-pj-",
    "outputId": "1594ec87-9475-4fbc-9612-5801de8609f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "[Microsoft, Corp., the, for, the, December, 31, 2021, as, to, the, corresponding, of, last, fiscal, Revenue, was, $, 51.7, billion, and, 20, was, $, 22.2, billion, and, 24, Net, was, $, 18.8, billion, and, 21, per, was, $, 2.48, and, 22, Digital, is, the, most, malleable, at, the, ’s, to, and, everyday, and, Satya, Nadella, and, chief, executive, of, Microsoft, As, tech, as, a, of, global, GDP, to, we, are, and, across, diverse, and, with, a, common, and, an, that, a, common, and, of, Solid, commercial, by, strong, by, long, Azure, Microsoft, Cloud, to, $, 22.1, billion, up, 32, over, Amy, Hood, executive, and, chief, financial, of, Microsoft]\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_tokens))\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABATZ6cY--Lh"
   },
   "source": [
    "## get the count of each pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hI5c0XVW-524",
    "outputId": "316fdd43-6b7e-447e-afd4-46545ce296ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 13,\n",
       " 92: 46,\n",
       " 100: 24,\n",
       " 90: 9,\n",
       " 85: 16,\n",
       " 93: 16,\n",
       " 97: 27,\n",
       " 98: 1,\n",
       " 84: 20,\n",
       " 103: 10,\n",
       " 87: 6,\n",
       " 99: 5,\n",
       " 89: 12,\n",
       " 86: 3,\n",
       " 94: 3,\n",
       " 95: 2}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "k6ipTe5--_YM",
    "outputId": "79e373bd-ce67-489f-f729-c3e081210ee4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[96].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHvWiqHN_C9i",
    "outputId": "0d2770ad-3f41-4f51-bfa7-7b59c75549e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 13\n",
      "NOUN | 46\n",
      "VERB | 24\n",
      "DET | 9\n",
      "ADP | 16\n",
      "NUM | 16\n",
      "PUNCT | 27\n",
      "SCONJ | 1\n",
      "ADJ | 20\n",
      "SPACE | 10\n",
      "AUX | 6\n",
      "SYM | 5\n",
      "CCONJ | 12\n",
      "ADV | 3\n",
      "PART | 3\n",
      "PRON | 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text, \"|\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wusYiljA_J4a"
   },
   "source": [
    "## Apply pos using ntlk not spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nn2fOT3q_EGD",
    "outputId": "d1591c17-df83-49c2-c120-4a23f2a90f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DT\n",
      "quick JJ\n",
      "brown NN\n",
      "fox NN\n",
      "jumps VBZ\n",
      "over IN\n",
      "the DT\n",
      "lazy JJ\n",
      "dog NN\n",
      ". .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging on the tokenized words\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Print the POS tags for each word\n",
    "for word, tag in pos_tags:\n",
    "    print(word, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwNIBDuUAFX0"
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3xKCWZV_M2r",
    "outputId": "9b8a67d9-8c8b-47d8-ee60-51cbc5631d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
    "\"\"\"\n",
    "Person Eg: Krish C Naik\n",
    "Place Or Location Eg: India\n",
    "Date Eg: September,24-09-1989\n",
    "Time  Eg: 4:30pm\n",
    "Money Eg: 1 million dollar\n",
    "Organization Eg: iNeuron Private Limited\n",
    "Percent Eg: 20%, twenty percent\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OqPv8xcAIcr"
   },
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures. Meta is a social media and social networking service owned by American technology conglomerate Meta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1758129307315,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "9NGGcqvfrCWS",
    "outputId": "cd304b4e-129c-4183-b119-912f86949c72"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures. Meta is a social media and social networking service owned by American technology conglomerate Meta'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2z0hcfgBp7T"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPQi-uRFBs2G",
    "outputId": "352e3f8d-c198-4546-e073-f50ff075c40f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower  |  FAC  |  Buildings, airports, highways, bridges, etc.\n",
      "1887 to 1889  |  DATE  |  Absolute or relative dates or periods\n",
      "Gustave Eiffel  |  PERSON  |  People, including fictional\n",
      "Meta  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "American  |  NORP  |  Nationalities or religious or political groups\n",
      "Meta  |  ORG  |  Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(sentence)\n",
    "for ent in doc1.ents:\n",
    "    print(ent ,\" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "t_GL5q3zBuj9",
    "outputId": "50312446-6e85-4e1f-f703-4b95f496c8be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Eiffel Tower\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " was built from \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1887 to 1889\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Gustave Eiffel\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", whose company specialized in building metal frameworks and structures. \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Meta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is a social media and social networking service owned by \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " technology conglomerate \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Meta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc1, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGDI-qHn9oGN"
   },
   "source": [
    "## Full English Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpM5c_Nq7ae3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srMaxJBU7ah1"
   },
   "outputs": [],
   "source": [
    "for resource in [\"punkt\", \"punkt_tab\", \"stopwords\", \"wordnet\", \"averaged_perceptron_tagger\",\"averaged_perceptron_tagger_eng\"]:\n",
    "    nltk.download(resource, quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT1-zBfz7akn"
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
    "the world have come and invaded us, captured our lands, conquered our minds.\n",
    "From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "the French, the Dutch, all of them came and looted us, took over what was ours.\n",
    "Yet we have not done this to any other nation. We have not conquered anyone.\n",
    "We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.\n",
    "Why? Because we respect the freedom of others.That is why my\n",
    "first vision is that of freedom. I believe that India got its first vision of\n",
    "this in 1857, when we started the War of Independence. It is this freedom that\n",
    "we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "I have a third vision. India must stand up to the world. Because I believe that unless India\n",
    "stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
    "strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
    "My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
    "space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "I see four milestones in my career\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_CM_rJR7oN9"
   },
   "outputs": [],
   "source": [
    "paragraph = paragraph.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qr9ut5QI7oQt"
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1758116896989,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "7FMRv-ii7oT2",
    "outputId": "6e1906b1-5f90-4611-d1a6-ea9685a82113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'three', 'visions', 'for', 'india', '.', 'in', '3000', 'years', 'of', 'our', 'history', ',', 'people', 'from', 'all', 'over', 'the', 'world', 'have', 'come', 'and', 'invaded', 'us', ',', 'captured', 'our', 'lands', ',', 'conquered', 'our', 'minds', '.', 'from', 'alexander', 'onwards', ',', 'the', 'greeks', ',', 'the', 'turks', ',', 'the', 'moguls', ',', 'the', 'portuguese', ',']\n"
     ]
    }
   ],
   "source": [
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758116924710,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "nhLWRPNU7yKP",
    "outputId": "7250b4d7-5ac2-4f3e-ec4e-c5a1dd45419c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1758116905561,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "0EQ7g0Vo7oWp",
    "outputId": "6dede0e0-71bd-4ec9-913b-dcb01f5a3a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 الكلمات بعد إزالة stopwords والرموز:\n",
      "['three', 'visions', 'india', 'years', 'history', 'people', 'world', 'come', 'invaded', 'us', 'captured', 'lands', 'conquered', 'minds', 'alexander', 'onwards', 'greeks', 'turks', 'moguls', 'portuguese', 'british', 'french', 'dutch', 'came', 'looted', 'us', 'took', 'yet', 'done', 'nation', 'conquered', 'anyone', 'grabbed', 'land', 'culture', 'history', 'tried', 'enforce', 'way', 'life', 'respect', 'freedom', 'first', 'vision', 'freedom', 'believe', 'india', 'got', 'first', 'vision']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = [word for word in words\n",
    "                  if re.match(\"^[a-zA-Z]+$\", word)\n",
    "                  and word not in stop_words]\n",
    "\n",
    "print(\"\\n📌 الكلمات بعد إزالة stopwords والرموز:\")\n",
    "print(filtered_words[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1758116934062,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "2wjf4INK71Ro",
    "outputId": "ac291375-44df-49b3-9faf-5adfbec11718"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKCn9RR17oZP"
   },
   "outputs": [],
   "source": [
    "# 5) Lemmatization (مع POS Tagging)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLf8T3UQ771g"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PykTUL1v78GH"
   },
   "outputs": [],
   "source": [
    "tagged_words = pos_tag(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1758117012682,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "1Eo-oSuJ78Ig",
    "outputId": "dd75b464-53b4-4007-ee0e-821939b29d84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('three', 'CD'),\n",
       " ('visions', 'NNS'),\n",
       " ('india', 'VBP'),\n",
       " ('years', 'NNS'),\n",
       " ('history', 'NN'),\n",
       " ('people', 'NNS'),\n",
       " ('world', 'NN'),\n",
       " ('come', 'VBP'),\n",
       " ('invaded', 'VBN'),\n",
       " ('us', 'PRP'),\n",
       " ('captured', 'JJ'),\n",
       " ('lands', 'NNS'),\n",
       " ('conquered', 'VBD'),\n",
       " ('minds', 'NNS'),\n",
       " ('alexander', 'VBP'),\n",
       " ('onwards', 'NNS'),\n",
       " ('greeks', 'NNS'),\n",
       " ('turks', 'NNS'),\n",
       " ('moguls', 'VBP'),\n",
       " ('portuguese', 'JJ'),\n",
       " ('british', 'JJ'),\n",
       " ('french', 'JJ'),\n",
       " ('dutch', 'NN'),\n",
       " ('came', 'VBD'),\n",
       " ('looted', 'JJ'),\n",
       " ('us', 'PRP'),\n",
       " ('took', 'VBD'),\n",
       " ('yet', 'RB'),\n",
       " ('done', 'VBN'),\n",
       " ('nation', 'NN'),\n",
       " ('conquered', 'VBD'),\n",
       " ('anyone', 'NN'),\n",
       " ('grabbed', 'JJ'),\n",
       " ('land', 'JJ'),\n",
       " ('culture', 'NN'),\n",
       " ('history', 'NN'),\n",
       " ('tried', 'VBD'),\n",
       " ('enforce', 'JJ'),\n",
       " ('way', 'NN'),\n",
       " ('life', 'NN'),\n",
       " ('respect', 'NN'),\n",
       " ('freedom', 'NN'),\n",
       " ('first', 'JJ'),\n",
       " ('vision', 'NN'),\n",
       " ('freedom', 'NN'),\n",
       " ('believe', 'VBP'),\n",
       " ('india', 'NN'),\n",
       " ('got', 'VBD'),\n",
       " ('first', 'JJ'),\n",
       " ('vision', 'NN'),\n",
       " ('started', 'VBD'),\n",
       " ('war', 'NN'),\n",
       " ('independence', 'NN'),\n",
       " ('freedom', 'NN'),\n",
       " ('must', 'MD'),\n",
       " ('protect', 'VB'),\n",
       " ('nurture', 'NN'),\n",
       " ('build', 'VB'),\n",
       " ('free', 'JJ'),\n",
       " ('one', 'CD'),\n",
       " ('respect', 'NN'),\n",
       " ('us', 'PRP'),\n",
       " ('second', 'JJ'),\n",
       " ('vision', 'NN'),\n",
       " ('india', 'NN'),\n",
       " ('development', 'NN'),\n",
       " ('fifty', 'JJ'),\n",
       " ('years', 'NNS'),\n",
       " ('developing', 'VBG'),\n",
       " ('nation', 'NN'),\n",
       " ('time', 'NN'),\n",
       " ('see', 'VB'),\n",
       " ('developed', 'JJ'),\n",
       " ('nation', 'NN'),\n",
       " ('among', 'IN'),\n",
       " ('top', 'JJ'),\n",
       " ('nations', 'NNS'),\n",
       " ('world', 'NN'),\n",
       " ('terms', 'NNS'),\n",
       " ('gdp', 'VBP'),\n",
       " ('percent', 'JJ'),\n",
       " ('growth', 'NN'),\n",
       " ('rate', 'NN'),\n",
       " ('areas', 'NNS'),\n",
       " ('poverty', 'NN'),\n",
       " ('levels', 'NNS'),\n",
       " ('falling', 'VBG'),\n",
       " ('achievements', 'NNS'),\n",
       " ('globally', 'RB'),\n",
       " ('recognised', 'VBN'),\n",
       " ('today', 'NN'),\n",
       " ('yet', 'RB'),\n",
       " ('lack', 'JJ'),\n",
       " ('see', 'NN'),\n",
       " ('developed', 'JJ'),\n",
       " ('nation', 'NN'),\n",
       " ('incorrect', 'JJ'),\n",
       " ('third', 'JJ'),\n",
       " ('vision', 'NN'),\n",
       " ('india', 'NN'),\n",
       " ('must', 'MD'),\n",
       " ('stand', 'VB'),\n",
       " ('world', 'NN'),\n",
       " ('believe', 'VBP')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words[:-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INchM5Bi78Kt"
   },
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag))   for word, tag in tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1758117083400,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "kdpmMTi778Nh",
    "outputId": "cc699f4f-2999-4644-edc2-d9b1181be024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three',\n",
       " 'vision',\n",
       " 'india',\n",
       " 'year',\n",
       " 'history',\n",
       " 'people',\n",
       " 'world',\n",
       " 'come',\n",
       " 'invade',\n",
       " 'u',\n",
       " 'captured',\n",
       " 'land',\n",
       " 'conquer',\n",
       " 'mind',\n",
       " 'alexander',\n",
       " 'onwards',\n",
       " 'greek',\n",
       " 'turk',\n",
       " 'moguls',\n",
       " 'portuguese',\n",
       " 'british',\n",
       " 'french',\n",
       " 'dutch',\n",
       " 'come',\n",
       " 'looted',\n",
       " 'u',\n",
       " 'take',\n",
       " 'yet',\n",
       " 'do',\n",
       " 'nation',\n",
       " 'conquer',\n",
       " 'anyone',\n",
       " 'grabbed',\n",
       " 'land',\n",
       " 'culture',\n",
       " 'history',\n",
       " 'try',\n",
       " 'enforce',\n",
       " 'way',\n",
       " 'life',\n",
       " 'respect',\n",
       " 'freedom',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'freedom',\n",
       " 'believe',\n",
       " 'india',\n",
       " 'get',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'start',\n",
       " 'war',\n",
       " 'independence',\n",
       " 'freedom',\n",
       " 'must',\n",
       " 'protect',\n",
       " 'nurture',\n",
       " 'build',\n",
       " 'free',\n",
       " 'one',\n",
       " 'respect',\n",
       " 'u',\n",
       " 'second',\n",
       " 'vision',\n",
       " 'india',\n",
       " 'development',\n",
       " 'fifty',\n",
       " 'year',\n",
       " 'develop',\n",
       " 'nation',\n",
       " 'time',\n",
       " 'see',\n",
       " 'developed',\n",
       " 'nation',\n",
       " 'among',\n",
       " 'top',\n",
       " 'nation',\n",
       " 'world',\n",
       " 'term',\n",
       " 'gdp',\n",
       " 'percent',\n",
       " 'growth',\n",
       " 'rate',\n",
       " 'area',\n",
       " 'poverty',\n",
       " 'level',\n",
       " 'fall',\n",
       " 'achievement',\n",
       " 'globally',\n",
       " 'recognise',\n",
       " 'today',\n",
       " 'yet',\n",
       " 'lack',\n",
       " 'see',\n",
       " 'developed',\n",
       " 'nation',\n",
       " 'incorrect',\n",
       " 'third',\n",
       " 'vision',\n",
       " 'india',\n",
       " 'must',\n",
       " 'stand',\n",
       " 'world',\n",
       " 'believe',\n",
       " 'unless',\n",
       " 'india',\n",
       " 'stand',\n",
       " 'world',\n",
       " 'one',\n",
       " 'respect',\n",
       " 'u',\n",
       " 'strength',\n",
       " 'respect',\n",
       " 'strength',\n",
       " 'must',\n",
       " 'strong',\n",
       " 'military',\n",
       " 'power',\n",
       " 'also',\n",
       " 'economic',\n",
       " 'power',\n",
       " 'must',\n",
       " 'go',\n",
       " 'good',\n",
       " 'fortune',\n",
       " 'work',\n",
       " 'three',\n",
       " 'great',\n",
       " 'mind',\n",
       " 'vikram',\n",
       " 'sarabhai',\n",
       " 'dept',\n",
       " 'space',\n",
       " 'professor',\n",
       " 'satish',\n",
       " 'dhawan',\n",
       " 'succeed',\n",
       " 'brahm',\n",
       " 'prakash',\n",
       " 'father',\n",
       " 'nuclear',\n",
       " 'material',\n",
       " 'lucky',\n",
       " 'work',\n",
       " 'three',\n",
       " 'closely',\n",
       " 'consider',\n",
       " 'great',\n",
       " 'opportunity',\n",
       " 'life',\n",
       " 'see',\n",
       " 'four',\n",
       " 'milestone',\n",
       " 'career']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1758117262907,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "hLuDrMFW78Qa",
    "outputId": "53ba9b2f-86f6-4f0f-f05e-779f8dde679d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT', 'The'),\n",
       " ('cats', 'NNS', 'cat'),\n",
       " ('are', 'VBP', 'be'),\n",
       " ('running', 'VBG', 'run'),\n",
       " ('better', 'JJR', 'good'),\n",
       " ('than', 'IN', 'than'),\n",
       " ('the', 'DT', 'the'),\n",
       " ('dogs', 'NNS', 'dog')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The cats are running better than the dogs\"\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [(word, tag, lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
    "              for word, tag in tagged]\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1758117378347,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "zx5617Vx9YS5",
    "outputId": "2646fa69-8738-44ee-85c6-e7d4536c359b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " comparison Stemming vs Lemmatization:\n",
      "three           | Stem: three      | Lemma: three\n",
      "visions         | Stem: vision     | Lemma: vision\n",
      "india           | Stem: india      | Lemma: india\n",
      "years           | Stem: year       | Lemma: year\n",
      "history         | Stem: histori    | Lemma: history\n",
      "people          | Stem: peopl      | Lemma: people\n",
      "world           | Stem: world      | Lemma: world\n",
      "come            | Stem: come       | Lemma: come\n",
      "invaded         | Stem: invad      | Lemma: invade\n",
      "us              | Stem: us         | Lemma: u\n",
      "captured        | Stem: captur     | Lemma: captured\n",
      "lands           | Stem: land       | Lemma: land\n",
      "conquered       | Stem: conquer    | Lemma: conquer\n",
      "minds           | Stem: mind       | Lemma: mind\n",
      "alexander       | Stem: alexand    | Lemma: alexander\n",
      "onwards         | Stem: onward     | Lemma: onwards\n",
      "greeks          | Stem: greek      | Lemma: greek\n",
      "turks           | Stem: turk       | Lemma: turk\n",
      "moguls          | Stem: mogul      | Lemma: moguls\n",
      "portuguese      | Stem: portugues  | Lemma: portuguese\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "print(\"\\n comparison Stemming vs Lemmatization:\")\n",
    "for i in range(20):\n",
    "    print(f\"{filtered_words[i]:15} | Stem: {stems[i]:10} | Lemma: {lemmatized_words[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758117448460,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "lrNY_NuD9rVW",
    "outputId": "b791bddc-568b-4a1c-8e97-30d04a6538c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Text\n",
      "three vision india year history people world come invade u captured land conquer mind alexander onwards greek turk moguls portuguese british french dutch come looted u take yet do nation conquer anyone grabbed land culture history try enforce way life respect freedom first vision freedom believe india get first vision start war independence freedom must protect nurture build free one respect u second vision india development fifty year develop nation time see developed nation among top nation wo ...\n",
      "\n",
      " Saved in cleaned_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "processed_paragraph = ' '.join(lemmatized_words)\n",
    "print(\"\\n Final Text\")\n",
    "print(processed_paragraph[:500], \"...\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"original\": filtered_words,\n",
    "    \"lemma\": lemmatized_words,\n",
    "    \"stem\": stems\n",
    "})\n",
    "df.to_csv(\"cleaned_tokens.csv\", index=False)\n",
    "print(\"\\n Saved in cleaned_tokens.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oFX6HTp_J-n"
   },
   "source": [
    "## Full Arabic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1758117707036,
     "user": {
      "displayName": "Baraa Abu Sallout",
      "userId": "01476061420378656318"
     },
     "user_tz": -180
    },
    "id": "sGvllq6696kw",
    "outputId": "f6bd6502-8ec8-4558-9c15-68dc287e05a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Words before cleaning:\n",
      "['التعليم', 'هو', 'أساس', 'تقدم', 'أي', 'مجتمع', '.', 'عندما', 'يستثمر', 'الناس', 'في', 'العلم', 'والمعرفة،', 'فإنهم', 'يفتحون', 'أبواب', 'المستقبل', 'أمام', 'الأجيال', 'القادمة', '.', 'في', 'السنوات', 'الأخيرة،', 'شهدنا', 'تطوراً', 'كبيراً', 'في', 'مجال', 'الذكاء', 'الاصطناعي،', 'وأصبح', 'جزءاً', 'أساسياً', 'من', 'حياتنا', 'اليومية،', 'ابتداءً', 'من', 'الهواتف', 'الذكية', 'وصولاً', 'إلى', 'تطبيقات', 'الرعاية', 'الصحية', '.', 'ومع', 'ذلك،', 'يبقى', 'التحدي', 'الأكبر', 'هو', 'كيفية', 'استخدام', 'هذه', 'التقنيات', 'بشكل', 'مسؤول', 'يضمن', 'العدالة', 'والشفافية', 'ويخدم', 'الإنسانية', 'بأفضل', 'صورة', '.']\n",
      "67\n",
      "\n",
      "📌 Words after removing stopwords and symbols:\n",
      "['التعليم', 'أساس', 'تقدم', 'أي', 'مجتمع', 'عندما', 'يستثمر', 'الناس', 'العلم', 'فإنهم', 'يفتحون', 'أبواب', 'المستقبل', 'أمام', 'الأجيال', 'القادمة', 'السنوات', 'شهدنا', 'مجال', 'الذكاء', 'وأصبح', 'حياتنا', 'الهواتف', 'الذكية', 'تطبيقات', 'الرعاية', 'الصحية', 'ومع', 'يبقى', 'التحدي', 'الأكبر', 'كيفية', 'استخدام', 'التقنيات', 'بشكل', 'مسؤول', 'يضمن', 'العدالة', 'والشفافية', 'ويخدم', 'الإنسانية', 'بأفضل', 'صورة']\n",
      "43\n",
      "\n",
      "📌 Example: Stemming Arabic words\n",
      "التعليم         | Stem: علم\n",
      "أساس            | Stem: اسس\n",
      "تقدم            | Stem: قدم\n",
      "أي              | Stem: اي\n",
      "مجتمع           | Stem: جمع\n",
      "عندما           | Stem: عند\n",
      "يستثمر          | Stem: ثمر\n",
      "الناس           | Stem: ناس\n",
      "العلم           | Stem: علم\n",
      "فإنهم           | Stem: فإن\n",
      "يفتحون          | Stem: فتح\n",
      "أبواب           | Stem: بوب\n",
      "المستقبل        | Stem: قبل\n",
      "أمام            | Stem: أمام\n",
      "الأجيال         | Stem: جيل\n",
      "\n",
      "📌 Final processed Arabic text:\n",
      "علم اسس قدم اي جمع عند ثمر ناس علم فإن فتح بوب قبل أمام جيل قدم سنو شهد جال ذكء أصبح حيت هتف ذكة طبق رعي صحة ومع بقى تحد كبر كيف خدم تقن شكل سؤل يضم عدل شفف خدم سان أفضل صور\n",
      "\n",
      "✅ Results saved to cleaned_tokens_ar.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 🟢 1) Import libraries\n",
    "# ================================\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary resources\n",
    "for resource in [\"punkt\", \"punkt_tab\"]:\n",
    "    nltk.download(resource, quiet=True)\n",
    "\n",
    "# ================================\n",
    "# 🟡 2) Define Arabic text\n",
    "# ================================\n",
    "paragraph = \"\"\"التعليم هو أساس تقدم أي مجتمع. عندما يستثمر الناس في العلم والمعرفة،\n",
    "فإنهم يفتحون أبواب المستقبل أمام الأجيال القادمة. في السنوات الأخيرة،\n",
    "شهدنا تطوراً كبيراً في مجال الذكاء الاصطناعي، وأصبح جزءاً أساسياً من حياتنا اليومية،\n",
    "ابتداءً من الهواتف الذكية وصولاً إلى تطبيقات الرعاية الصحية. ومع ذلك،\n",
    "يبقى التحدي الأكبر هو كيفية استخدام هذه التقنيات بشكل مسؤول يضمن\n",
    "العدالة والشفافية ويخدم الإنسانية بأفضل صورة.\n",
    "\"\"\"\n",
    "\n",
    "# ================================\n",
    "# 🔵 3) Tokenization (split into words)\n",
    "# ================================\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "print(\"📌 Words before cleaning:\")\n",
    "print(words)\n",
    "print(len(words))\n",
    "\n",
    "# ================================\n",
    "# 🟣 4) Remove stopwords + symbols\n",
    "# ================================\n",
    "# Custom Arabic stopwords (can be extended)\n",
    "arabic_stopwords = set([\n",
    "    \"في\",\"من\",\"على\",\"إلى\",\"عن\",\"مع\",\"كان\",\"التي\",\"الذي\",\n",
    "    \"هذا\",\"هذه\",\"ذلك\",\"تلك\",\"هناك\",\"كل\",\"لم\",\"لن\",\"ما\",\n",
    "    \"إن\",\"أن\",\"قد\",\"حتى\",\"ثم\",\"به\",\"بها\",\"هو\",\"هي\",\"هم\",\"هن\",\n",
    "    \"أنا\",\"نحن\",\"انت\",\"أنت\",\"انتم\",\"انتن\"\n",
    "])\n",
    "\n",
    "# stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "\n",
    "# Keep only Arabic letters and remove stopwords\n",
    "filtered_words = [word for word in words\n",
    "                  if re.match(\"^[\\u0621-\\u064A]+$\", word)\n",
    "                  and word not in arabic_stopwords]\n",
    "\n",
    "print(\"\\n📌 Words after removing stopwords and symbols:\")\n",
    "print(filtered_words)\n",
    "print(len(filtered_words))\n",
    "\n",
    "# ================================\n",
    "# 🟤 5) (Optional) Stemming with ISRIStemmer\n",
    "# ================================\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "stems = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "print(\"\\n📌 Example: Stemming Arabic words\")\n",
    "for i in range(min(15, len(filtered_words))):\n",
    "    print(f\"{filtered_words[i]:15} | Stem: {stems[i]}\")\n",
    "\n",
    "# ================================\n",
    "# 🔴 6) Final cleaned text + Save\n",
    "# ================================\n",
    "processed_paragraph = \" \".join(stems)\n",
    "print(\"\\n📌 Final processed Arabic text:\")\n",
    "print(processed_paragraph)\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame({\n",
    "    \"original\": filtered_words,\n",
    "    \"stem\": stems\n",
    "})\n",
    "df.to_csv(\"cleaned_tokens_ar.csv\", index=False)\n",
    "print(\"\\n✅ Results saved to cleaned_tokens_ar.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viPqJuSysvF3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3pSMgVjVio_"
   },
   "source": [
    "#  Lab 1 NLP Text Preprocessing Assignment (NLTK + SpyCy)\n",
    "\n",
    "## Objective\n",
    "This assignment aims to assess your understanding of **text preprocessing** and **linguistic analysis** techniques using Python and the NLTK library.\n",
    "\n",
    "You will apply multiple NLP steps to raw text data in preparation for **machine learning tasks** such as **text classification** or **sentiment analysis**.\n",
    "---\n",
    "\n",
    "## 📄 Given Text Data\n",
    "Use the following dataset in your assignment:\n",
    "\n",
    "```python\n",
    "text_data = [\n",
    "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
    "    \"I hated the film, it was the worst I have ever seen\",\n",
    "    \"The storyline was boring but the acting was brilliant\",\n",
    "    \"An amazing movie with a great plot and incredible performances\",\n",
    "    \"Egypt movie, I regret wasting my time on it\",\n",
    "    \"The actors did a great job but the story lacked depth\",\n",
    "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
    "    \"This film was just okay, not too bad but not great either\",\n",
    "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
    "    \"The movie was disappointing, it did not live up to the hype\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  Tasks\n",
    "\n",
    "### 1️⃣ Tokenization\n",
    "**Task:**\n",
    "- Convert all sentences to lowercase.\n",
    "- Tokenize each sentence into individual words.\n",
    "\n",
    "📌 *Hint:* Use `nltk.tokenize.word_tokenize`\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Stopword Removal\n",
    "**Task:**\n",
    "- Remove English stopwords from the tokenized text.\n",
    "- Remove punctuation and non-alphabetic tokens.\n",
    "\n",
    "📌 *Hint:* Use `nltk.corpus.stopwords`\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Stemming\n",
    "**Task:**\n",
    "- Apply stemming to the cleaned tokens.\n",
    "\n",
    "📌 *Hint:* Use `nltk.stem.PorterStemmer`\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Lemmatization\n",
    "**Task:**\n",
    "- Apply lemmatization to the cleaned tokens.\n",
    "\n",
    "📌 *Hint:* Use `nltk.stem.WordNetLemmatizer`\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ Part-of-Speech (POS) Tagging\n",
    "**Task:**\n",
    "- Perform POS tagging on the lemmatized tokens.\n",
    "\n",
    "📌 *Hint:* Use `nltk.pos_tag`\n",
    "\n",
    "---\n",
    "\n",
    "### 6️⃣ Named Entity Recognition (NER)\n",
    "**Task:**\n",
    "- Identify named entities in the text.\n",
    "- Focus on entities such as **locations**, **organizations**, or **names**.\n",
    "\n",
    "📌 *Hint:* Use `nltk.ne_chunk`\n",
    "\n",
    "---\n",
    "\n",
    "### 7️⃣ Save Results to CSV\n",
    "**Task:**\n",
    "- Create a pandas DataFrame containing:\n",
    "  - Original text\n",
    "  - Tokenized text\n",
    "  - Cleaned tokens\n",
    "  - Stemmed tokens\n",
    "  - Lemmatized tokens\n",
    "  - POS tags\n",
    "- Save the DataFrame to a CSV file named:\n",
    "\n",
    "```\n",
    "nlp_assignment_results.csv\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Happy Coding 💻\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVTnl6vkst1L"
   },
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
    "    \"I hated the film, it was the worst I have ever seen\",\n",
    "    \"The storyline was boring but the acting was brilliant\",\n",
    "    \"An amazing movie with a great plot and incredible performances\",\n",
    "    \"Egypt movie, I regret wasting my time on it\",\n",
    "    \"The actors did a great job but the story lacked depth\",\n",
    "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
    "    \"This film was just okay, not too bad but not great either\",\n",
    "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
    "    \"The movie was disappointing, it did not live up to the hype\"\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMQ2HNl9yTfP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kGDI-qHn9oGN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
