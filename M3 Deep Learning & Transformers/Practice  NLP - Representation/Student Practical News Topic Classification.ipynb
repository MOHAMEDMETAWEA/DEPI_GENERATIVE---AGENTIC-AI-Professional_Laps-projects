{"cells":[{"cell_type":"markdown","id":"1ea8e19e-8ea2-4f12-aac2-1d707a601220","metadata":{"id":"1ea8e19e-8ea2-4f12-aac2-1d707a601220"},"source":["# Task: News Topic Classification with AG News\n","\n","## Objective\n","Classify **news articles** into 4 categories (*World, Sports, Business, Sci/Tech*) using different **text representation methods**.\n","\n","<small>[Link: AG News Classification Dataset on Kaggle](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)</small>\n","    \n","---\n","\n","## Step 1: Data Preparation\n","- Load the **AG News dataset** (train.csv & test.csv).  \n","- Combine the **title + description** into one text field.  \n","- Apply **basic preprocessing** (depend of the task)\n","---\n","\n","## Step 2: Representations to Try\n","You must implement **all 4 methods** below:\n","\n","1. **One hot Encoding**  (NOW)\n","\n","2. **Bag of Words (BoW)**   (NOW)\n","   - Represent each text as a count of words.  \n","\n","\n","3. **TF-IDF**   (NOW)\n","   - Apply TF-IDF weighting instead of raw counts.  \n","\n","\n","4. **N-grams (Bi/Tri-grams)**   (NOW)\n","   - Use bigrams and trigrams to capture context.   \n","\n","## Step 3: Try Two Classifiers\n","For **each text representation method**, train **two different models** and compare:\n","\n","- **Logistic Regression**\n","- **Naive Bayes** (or any other model of your choice, e.g., SVM, Decision Tree)\n","\n","Hint:  \n","- Logistic Regression usually performs well on sparse features (BoW, TF-IDF, N-grams).  \n","- Naive Bayes is very fast and works surprisingly well for text classification.  \n","- Compare their accuracy for each representation.\n","\n","---\n","\n","## Step 4: Ali Results Table\n","Fill in your results:\n","\n","| Representation | Logistic Regression Acc | Naive Bayes Acc | Notes |\n","|----------------|--------------------------|-----------------|-------|\n","| One Hot            |                          |                 |       |\n","| BoW            |                          |                 |       |\n","| TF-IDF         |                          |                 |       |\n","| N-grams        |                          |                 |       |\n","---\n","\n","## Reflection Questions\n","1. Which method gave the best accuracy? Why?  \n","2. Which method is more efficient in terms of speed and memory?  \n","3. If you had to build a **real news classifier**, which method would you choose and why?  \n"]},{"cell_type":"code","execution_count":null,"id":"d1161818-6c0d-4ebc-aff7-6a2e3b83ec9a","metadata":{"id":"d1161818-6c0d-4ebc-aff7-6a2e3b83ec9a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"de8adc83-1a58-4fe7-ae90-e9495345f3ff","metadata":{"id":"de8adc83-1a58-4fe7-ae90-e9495345f3ff"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"cd8050c1-f570-415c-b764-855818335834","metadata":{"id":"cd8050c1-f570-415c-b764-855818335834"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f6b64ff7-36a5-4223-afe1-ff96c6ecce07","metadata":{"id":"f6b64ff7-36a5-4223-afe1-ff96c6ecce07"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b1ae4b90-4f5e-4c59-8133-30a88df58cb1","metadata":{"id":"b1ae4b90-4f5e-4c59-8133-30a88df58cb1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"49e75f3a-6aeb-43af-85ee-383ddef5702e","metadata":{"id":"49e75f3a-6aeb-43af-85ee-383ddef5702e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ad0cfd2e-f61a-4b99-89d7-e94fbc9d776b","metadata":{"id":"ad0cfd2e-f61a-4b99-89d7-e94fbc9d776b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}