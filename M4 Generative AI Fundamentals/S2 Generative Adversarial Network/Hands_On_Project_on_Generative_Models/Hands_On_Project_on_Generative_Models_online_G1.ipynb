{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc422bc9",
   "metadata": {},
   "source": [
    "# Fashion Generative Studio\n",
    "## A Complete Hands-On Project on Generative Models\n",
    "\n",
    "---\n",
    "\n",
    "## Project Title\n",
    "**Fashion Generative Studio: Building and Comparing VAE, GANs Models**\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, participants will build a complete **Generative AI Studio** capable of producing new fashion images inspired by the Fashion-MNIST dataset.\n",
    "\n",
    "Students will implement and train three major families of generative models:\n",
    "\n",
    "1. **Variational AutoEncoder (VAE)**\n",
    "2. **Generative Adversarial Network (GAN / DCGAN)**\n",
    "\n",
    "All models will be trained on the same dataset and evaluated using the same metrics, enabling a fair and educational comparison.\n",
    "\n",
    "The goal is not only to generate images, but to deeply understand how each generative paradigm learns, how stable it is during training, and how its outputs differ in quality and diversity.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this project, students will:\n",
    "\n",
    "- Understand the concept of **latent representations**\n",
    "- Learn how **probabilistic encoders and decoders** work\n",
    "- Understand **adversarial training dynamics**\n",
    "- Gain experience with **training stability challenges**\n",
    "- Learn how to **evaluate generative models quantitatively**\n",
    "- Build a complete reproducible generative AI pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**Fashion-MNIST**\n",
    "\n",
    "- 70,000 grayscale images\n",
    "- Image size: 28 × 28\n",
    "- 10 clothing categories\n",
    "- Well-suited for rapid experimentation\n",
    "- Visually interpretable for comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "The project is divided into five main phases:\n",
    "\n",
    "1. Environment and Data Preparation\n",
    "2. Variational AutoEncoder (VAE)\n",
    "3. Generative Adversarial Network (GAN)\n",
    "4. Evaluation and Final Comparison\n",
    "5. Generative Playground (Final Demo)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Fashion-MNIST Label Map\n",
    "| Label | Clothing Item |\n",
    "|--------|--------------|\n",
    "| 0 | T-shirt / Top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439f7a7",
   "metadata": {},
   "source": [
    "# Phase 1 — Environment and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e769886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Environment and Data Preparation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('studio_outputs', exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data preparation\n",
    "batch_size = 128\n",
    "\n",
    "# VAE uses [0, 1] normalization\n",
    "transform_vae = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# GAN uses [-1, 1] normalization\n",
    "transform_gan = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Fashion-MNIST\n",
    "train_dataset_vae = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_vae)\n",
    "train_dataset_gan = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_gan)\n",
    "\n",
    "train_loader_vae = DataLoader(train_dataset_vae, batch_size=batch_size, shuffle=True)\n",
    "train_loader_gan = DataLoader(train_dataset_gan, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Visualize real samples\n",
    "real_batch, _ = next(iter(train_loader_vae))\n",
    "grid = utils.make_grid(real_batch[:64], nrow=8, normalize=True)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(np.transpose(grid), cmap='gray')\n",
    "plt.title(\"Real Samples\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943a3b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Goal\n",
    "Establish a unified environment and data pipeline for all models.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Install required libraries (Python, PyTorch, Torchvision)\n",
    "2. Download the Fashion-MNIST dataset\n",
    "3. Convert images to tensors\n",
    "4. Normalize data:\n",
    "   - VAE: values in [0, 1]\n",
    "   - GAN & Diffusion: values in [-1, 1]\n",
    "5. Create DataLoaders with fixed batch size\n",
    "6. Save a reference grid of real images for later comparison\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "- Functional DataLoader\n",
    "- Saved grid of real Fashion-MNIST samples\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b576d37",
   "metadata": {},
   "source": [
    "# Phase 2 — Variational AutoEncoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Variational AutoEncoder (VAE) Implementation\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x.view(-1, 784))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z).view(-1, 1, 28, 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "vae_latent_dim = 20\n",
    "vae_model = VAE(vae_latent_dim).to(device)\n",
    "vae_optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    vae_model.train()\n",
    "    total_loss = 0\n",
    "    for data, _ in train_loader_vae:\n",
    "        data = data.to(device)\n",
    "        vae_optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae_model(data)\n",
    "        loss = vae_loss(recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"VAE Epoch {epoch+1}, Avg Loss: {total_loss/len(train_loader_vae.dataset):.4f}\")\n",
    "\n",
    "# Visualization\n",
    "vae_model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, vae_latent_dim).to(device)\n",
    "    samples = vae_model.decode(z).cpu()\n",
    "    grid = utils.make_grid(samples, nrow=8, normalize=True)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(np.transpose(grid), cmap='gray')\n",
    "    plt.title(\"VAE Generated Samples\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c9d8d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Goal\n",
    "Learn structured latent representations and image reconstruction.\n",
    "\n",
    "## Concept Summary\n",
    "\n",
    "A VAE consists of:\n",
    "\n",
    "- **Encoder**: maps input image to mean and variance vectors\n",
    "- **Reparameterization Trick**: samples latent vector from distribution\n",
    "- **Decoder**: reconstructs image from latent vector\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Build an Encoder network that outputs mean and variance\n",
    "2. Apply reparameterization to sample latent vector\n",
    "3. Build a Decoder network that reconstructs images\n",
    "4. Define the VAE loss function:\n",
    "   - Reconstruction Loss\n",
    "   - KL Divergence\n",
    "5. Train the VAE model on Fashion-MNIST\n",
    "6. Visualize reconstructed images vs original\n",
    "7. Generate new samples from random latent vectors\n",
    "8. Explore latent space by varying single latent dimensions\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- Reconstruction comparison grid\n",
    "- Generated image samples\n",
    "- Latent traversal visualization\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d4a63",
   "metadata": {},
   "source": [
    "# Phase 3 — Generative Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Generative Adversarial Network (GAN) Implementation\n",
    "nz = 100\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, 256, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 3, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 7, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 1, 1, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1)\n",
    "\n",
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    for i, (data, _) in enumerate(train_loader_gan):\n",
    "        netD.zero_grad()\n",
    "        real = data.to(device)\n",
    "        b_size = real.size(0)\n",
    "        label = torch.full((b_size,), 1.0, device=device)\n",
    "        output = netD(real).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        \n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(0.0)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        label.fill_(1.0)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "    print(f\"GAN Epoch {epoch+1}, Loss_D: {errD_real+errD_fake:.4f}, Loss_G: {errG:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "netG.eval()\n",
    "with torch.no_grad():\n",
    "    fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "    fake = netG(fixed_noise).cpu()\n",
    "    grid = utils.make_grid(fake, nrow=8, normalize=True)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(np.transpose(grid), cmap='gray')\n",
    "    plt.title(\"GAN Generated Samples\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6fe7d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Goal\n",
    "Learn image generation through adversarial training.\n",
    "\n",
    "## Concept Summary\n",
    "\n",
    "A GAN consists of:\n",
    "\n",
    "- **Generator**: produces fake images from random noise\n",
    "- **Discriminator**: distinguishes real images from fake ones\n",
    "- Both networks play a minimax adversarial game\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Build the Generator network\n",
    "2. Build the Discriminator network\n",
    "3. Define adversarial loss function\n",
    "4. Train the Discriminator on real and fake images\n",
    "5. Train the Generator to fool the Discriminator\n",
    "6. Monitor training stability and losses\n",
    "7. Save generated samples at regular intervals\n",
    "8. Detect and discuss mode collapse if it occurs\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- Progressive generation GIF\n",
    "- Generator and Discriminator loss curves\n",
    "- Final generated image grid\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93122a9f",
   "metadata": {},
   "source": [
    "# Phase 4 — Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Evaluation and Comparison\n",
    "class Evaluator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Evaluator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*11*11, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "eval_net = Evaluator().to(device)\n",
    "eval_opt = optim.Adam(eval_net.parameters(), lr=0.001)\n",
    "eval_crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train evaluator on real data\n",
    "for epoch in range(5):\n",
    "    for data, target in train_loader_vae:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        eval_opt.zero_grad()\n",
    "        loss = eval_crit(eval_net(data), target)\n",
    "        loss.backward()\n",
    "        eval_opt.step()\n",
    "print(\"Evaluator trained.\")\n",
    "\n",
    "def get_stats(model, model_type='vae'):\n",
    "    eval_net.eval()\n",
    "    with torch.no_grad():\n",
    "        if model_type == 'vae':\n",
    "            z = torch.randn(1000, vae_latent_dim).to(device)\n",
    "            samples = model.decode(z)\n",
    "        else:\n",
    "            z = torch.randn(1000, nz, 1, 1, device=device)\n",
    "            samples = (model(z) + 1) / 2\n",
    "        \n",
    "        logits = eval_net(samples)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        max_probs, _ = torch.max(probs, dim=1)\n",
    "        return max_probs.mean().item()\n",
    "\n",
    "vae_conf = get_stats(vae_model, 'vae')\n",
    "gan_conf = get_stats(netG, 'gan')\n",
    "\n",
    "print(f\"VAE Avg Confidence: {vae_conf:.4f}\")\n",
    "print(f\"GAN Avg Confidence: {gan_conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa753e8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Goal\n",
    "Quantitatively and visually compare all models.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Train a simple classifier on real Fashion-MNIST\n",
    "2. Generate large image sets from each model (VAE,GANs)\n",
    "3. Compute evaluation metrics:\n",
    "   - Fréchet Inception Distance (FID)\n",
    "   - Sample diversity per class\n",
    "4. Measure training and sampling time\n",
    "5. Create a comparison table\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- Evaluation report\n",
    "- Quantitative comparison table\n",
    "- Visual comparison gallery\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03080218",
   "metadata": {},
   "source": [
    "# Phase 5 — Generative Playground (Final Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Generative Playground Demo\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def preview(model_type, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    with torch.no_grad():\n",
    "        if model_type == 'VAE':\n",
    "            z = torch.randn(1, vae_latent_dim).to(device)\n",
    "            img = vae_model.decode(z).view(28, 28).cpu().numpy()\n",
    "        else:\n",
    "            z = torch.randn(1, nz, 1, 1, device=device)\n",
    "            img = netG(z).view(28, 28).cpu().numpy()\n",
    "            img = (img + 1) / 2\n",
    "    \n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"{model_type} (Seed {seed})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "model_drop = widgets.Dropdown(options=['VAE', 'GAN'], description='Model:')\n",
    "seed_input = widgets.IntSlider(min=0, max=999, description='Seed:')\n",
    "ui = widgets.HBox([model_drop, seed_input])\n",
    "out = widgets.interactive_output(preview, {'model_type': model_drop, 'seed': seed_input})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59d00e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Goal\n",
    "Build an interactive interface that allows real-time generation and comparison of images from VAE, GAN models.\n",
    "This phase transforms the trained models into a live demonstration tool, enabling users to explore generative behavior through simple controls.\n",
    "\n",
    "---\n",
    "## Concept Summary\n",
    "\n",
    "The Generative Playground is a lightweight application where users can:\n",
    "\n",
    "- Select a generative model (VAE, GAN)\n",
    "- Adjust latent or noise parameters\n",
    "- Change random seed values\n",
    "- Instantly visualize newly generated samples\n",
    "\n",
    "This creates a practical showcase of how different generative paradigms respond to input changes.\n",
    "---\n",
    "\n",
    "## Interface Requirements\n",
    "\n",
    "The interface should provide:\n",
    "- Model selection dropdown (VAE / GAN)\n",
    "- Seed input field\n",
    "- Latent slider (for VAE exploration)\n",
    "- Generate button\n",
    "- Image grid output area\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Implementation Options\n",
    "\n",
    "### Option 1 — Streamlit App\n",
    "- Simple web-based interface\n",
    "- Runs locally or on cloud\n",
    "- Interactive sliders and buttons\n",
    "- Automatic refresh of generated samples\n",
    "\n",
    "### Option 2 — Interactive Notebook\n",
    "\n",
    "- Jupyter widgets for controls\n",
    "- Inline image display\n",
    "- Suitable for classroom demonstrations\n",
    "\n",
    "---\n",
    "\n",
    "## Functional Workflow\n",
    "\n",
    "1. Load trained model checkpoints\n",
    "2. Wait for user selection\n",
    "3. Generate latent/noise vector using chosen seed\n",
    "4. Run sampling pipeline\n",
    "5. Display generated images immediately\n",
    "\n",
    "---\n",
    "\n",
    "## User Interactions\n",
    "\n",
    "- Changing seed → produces different random samples\n",
    "- Changing latent slider → explores VAE latent space\n",
    "---\n",
    "# Final Presentation\n",
    "\n",
    "Each team presents:\n",
    "\n",
    "- Explanation of how each model learns\n",
    "- Challenges faced during training\n",
    "- Comparison of outputs\n",
    "- Key lessons learned\n",
    "\n",
    "---\n",
    "\n",
    "# Expected Outcomes\n",
    "\n",
    "After completing this project, students will be able to:\n",
    "\n",
    "- Understand modern generative AI systems\n",
    "- Implement and train generative models\n",
    "- Diagnose training instability\n",
    "- Evaluate generative models objectively\n",
    "- Transition confidently to large-scale diffusion models such as Stable Diffusion\n",
    "\n",
    "---\n",
    "\n",
    "# Project Value\n",
    "\n",
    "This project bridges theory and practice in Generative AI, giving students strong foundations to understand and build real-world generative systems.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
