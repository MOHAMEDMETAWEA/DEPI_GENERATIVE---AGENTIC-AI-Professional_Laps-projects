{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stable Diffusion Inference & Generation Control (Local Version)\n",
                "\n",
                "This notebook has been adapted for local execution, supporting both CPU and CUDA (GPU) devices."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "from diffusers import DiffusionPipeline\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from contextlib import nullcontext\n",
                "\n",
                "# Device Configuration\n",
                "if torch.cuda.is_available():\n",
                "    device = \"cuda\"\n",
                "    dtype = torch.float16\n",
                "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    device = \"cpu\"\n",
                "    dtype = torch.float32\n",
                "    print(\"Using CPU. Note: Generation will be slow.\")\n",
                "\n",
                "model_path = \"./sd15_local\"\n",
                "model_id = \"runwayml/stable-diffusion-v1-5\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load or Download Model\n",
                "if not os.path.exists(model_path):\n",
                "    print(f\"Downloading model '{model_id}' ...\")\n",
                "    pipe = DiffusionPipeline.from_pretrained(\n",
                "        model_id,\n",
                "        torch_dtype=dtype\n",
                "    )\n",
                "    pipe.save_pretrained(model_path)\n",
                "    print(f\"Model saved to {model_path}\")\n",
                "else:\n",
                "    print(f\"Loading model from {model_path} ...\")\n",
                "    pipe = DiffusionPipeline.from_pretrained(\n",
                "        model_path,\n",
                "        torch_dtype=dtype\n",
                "    )\n",
                "\n",
                "pipe = pipe.to(device)\n",
                "print(\"Stable Diffusion ready \\ud83d\\ude80\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_image(pipe, prompt, seed=None, negative_prompt=None, steps=25, cfg=7.5):\n",
                "    if seed is None:\n",
                "        seed = torch.seed()\n",
                "    \n",
                "    # Use a fixed generator for reproducible results if seed is provided\n",
                "    generator = torch.Generator(device=device).manual_seed(seed)\n",
                "    \n",
                "    # Autocast only for CUDA\n",
                "    amp_ctx = torch.autocast(\"cuda\") if device == \"cuda\" else nullcontext()\n",
                "\n",
                "    with torch.inference_mode(), amp_ctx:\n",
                "        result = pipe(\n",
                "            prompt=prompt,\n",
                "            negative_prompt=negative_prompt,\n",
                "            num_inference_steps=steps,\n",
                "            guidance_scale=cfg,\n",
                "            generator=generator,\n",
                "        )\n",
                "    return result.images[0]\n",
                "\n",
                "def show_two_images(img1, title1, img2, title2):\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    \n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.imshow(img1)\n",
                "    plt.title(title1)\n",
                "    plt.axis(\"off\")\n",
                "    \n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.imshow(img2)\n",
                "    plt.title(title2)\n",
                "    plt.axis(\"off\")\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Comparison: Randomness (No Seed)\n",
                "\n",
                "Generating two images with the same prompt but without a fixed seed results in different images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"A futuristic AI lab, holographic screens, ultra detailed\"\n",
                "\n",
                "print(\"Generating image 1...\")\n",
                "img1 = generate_image(pipe, prompt)\n",
                "print(\"Generating image 2...\")\n",
                "img2 = generate_image(pipe, prompt)\n",
                "\n",
                "show_two_images(img1, \"Random Seed (Run 1)\", img2, \"Random Seed (Run 2)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Comparison: Reproducibility (With Seed)\n",
                "\n",
                "Providing the same seed ensures the exact same image is generated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seed = 42\n",
                "\n",
                "print(\"Generating image 1 with fixed seed...\")\n",
                "img3 = generate_image(pipe, prompt, seed=seed)\n",
                "print(\"Generating image 2 with fixed seed...\")\n",
                "img4 = generate_image(pipe, prompt, seed=seed)\n",
                "\n",
                "show_two_images(img3, f\"Fixed Seed {seed} (Run 1)\", img4, f\"Fixed Seed {seed} (Run 2)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Influence of Negative Prompts\n",
                "\n",
                "Negative prompts allow you to specify what you *don't* want to see in the image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_nature = \"A beautiful mountain landscape with a clear lake, oil painting style\"\n",
                "neg_prompt = \"trees, forest, green color\"\n",
                "seed = 123\n",
                "\n",
                "print(\"Generating image without negative prompt...\")\n",
                "img5 = generate_image(pipe, prompt_nature, seed=seed)\n",
                "print(\"Generating image with negative prompt...\")\n",
                "img6 = generate_image(pipe, prompt_nature, seed=seed, negative_prompt=neg_prompt)\n",
                "\n",
                "show_two_images(img5, \"No Negative Prompt\", img6, \"With Negative Prompt (No trees)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. CFG Scale (Guidance Scale)\n",
                "\n",
                "CFG scale controls how closely the model follows the prompt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_cat = \"A cute cat wearing a space suit, digital art\"\n",
                "seed = 777\n",
                "\n",
                "print(\"Generating with CFG=2.0 (Lower guidance)...\")\n",
                "img7 = generate_image(pipe, prompt_cat, seed=seed, cfg=2.0)\n",
                "print(\"Generating with CFG=15.0 (Higher guidance)...\")\n",
                "img8 = generate_image(pipe, prompt_cat, seed=seed, cfg=15.0)\n",
                "\n",
                "show_two_images(img7, \"Low CFG (2.0)\", img8, \"High CFG (15.0)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}